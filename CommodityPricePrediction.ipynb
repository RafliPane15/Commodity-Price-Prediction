{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":92581,"databundleVersionId":11279751,"sourceType":"competition"},{"sourceId":10983135,"sourceType":"datasetVersion","datasetId":6835419},{"sourceId":10983286,"sourceType":"datasetVersion","datasetId":6835539}],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":1149.351139,"end_time":"2025-03-01T18:30:34.023793","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-03-01T18:11:24.672654","version":"2.6.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Commodity Price Prediction\n---\n## Overview\nSeiring dengan meningkatnya volatilitas pasar pangan global, memprediksi harga komoditas pangan dengan akurat menjadi tantangan penting bagi pembuat kebijakan, pelaku bisnis, dan peneliti. Kompetisi Prediksi Harga Komoditas Pangan ini memberikan kesempatan bagi peserta untuk menganalisis faktor-faktor yang mempengaruhi fluktuasi harga bahan pangan. Dataset yang digunakan mencakup tren harga historis, kondisi pasar, dan indikator ekonomi yang berperan dalam perubahan harga pangan di berbagai wilayah. Dengan menerapkan teknik data science, peserta dapat menggali wawasan mendalam tentang dinamika pasar yang mendasari pergerakan harga.\n\n## Permasalahan\nFluktuasi harga komoditas pangan memiliki dampak ekonomi dan sosial yang signifikan, mulai dari anggaran rumah tangga hingga kebijakan perdagangan global. Dalam kompetisi ini, peserta ditantang untuk mengembangkan model prediktif yang dapat memperkirakan harga komoditas pangan berdasarkan data historis. Dataset yang diberikan mencakup berbagai fitur seperti harga sebelumnya, nilai tukar mata uang, tren pasokan global, serta indikator makroekonomi. Dengan menggunakan data tersebut, peserta diharapkan dapat membangun model yang mampu memprediksi harga setiap komoditas tiap tanggal pada rentang waktu tertentu dengan tingkat akurasi yang tinggi.\n\n## Tujuan\nTujuan dari kompetisi ini adalah mendorong para data scientist untuk mengasah keterampilan mereka dalam membangun model prediksi yang akurat. Dengan berpartisipasi, peserta turut berkontribusi dalam pemahaman yang lebih mendalam mengenai pergerakan harga pangan dan dampaknya terhadap ketahanan pangan serta stabilitas ekonomi. Selain meningkatkan kemampuan teknis, kompetisi ini juga berfokus pada pentingnya pengambilan keputusan berbasis data dalam mengelola rantai pasokan pangan dan mengurangi risiko volatilitas harga.","metadata":{}},{"cell_type":"markdown","source":"# Importing Data and Libraries\n---","metadata":{"papermill":{"duration":0.015371,"end_time":"2025-03-01T18:11:27.733070","exception":false,"start_time":"2025-03-01T18:11:27.717699","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Basics\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Preprocessing\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import MinMaxScaler\nfrom datetime import timedelta\n# Machine Learning Models\nfrom xgboost import XGBRegressor, DMatrix\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn import svm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n# Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\n# Metrics\nfrom sklearn.metrics import mean_absolute_percentage_error\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, balanced_accuracy_score, accuracy_score\n# Hyperparameter Tuning\nimport optuna\n# Random State\nRANDOM_STATE = 1805\npd.set_option('display.max_columns', None)  # or 1000\npd.set_option(\"display.max_rows\", 50)","metadata":{"papermill":{"duration":14.879166,"end_time":"2025-03-01T18:11:42.626563","exception":false,"start_time":"2025-03-01T18:11:27.747397","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:11.792055Z","iopub.execute_input":"2025-03-11T16:57:11.792389Z","iopub.status.idle":"2025-03-11T16:57:35.322259Z","shell.execute_reply.started":"2025-03-11T16:57:11.792359Z","shell.execute_reply":"2025-03-11T16:57:35.321060Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Train/Test","metadata":{}},{"cell_type":"code","source":"train_path = '/kaggle/input/comodity-price-prediction-penyisihan-arkavidia-9/Harga Bahan Pangan/train'\ntest_path = '/kaggle/input/comodity-price-prediction-penyisihan-arkavidia-9/Harga Bahan Pangan/test'\nfilenames = os.listdir(train_path)\n\ntrain = dict()\ntest = dict()\nfor file in filenames:\n    print(file)\n    train[file] = pd.read_csv(os.path.join(train_path,file))\n    test[file] = pd.read_csv(os.path.join(test_path,file))\ntrain","metadata":{"papermill":{"duration":0.111865,"end_time":"2025-03-01T18:11:42.753516","exception":false,"start_time":"2025-03-01T18:11:42.641651","status":"completed"},"scrolled":true,"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:35.323851Z","iopub.execute_input":"2025-03-11T16:57:35.324689Z","iopub.status.idle":"2025-03-11T16:57:35.957536Z","shell.execute_reply.started":"2025-03-11T16:57:35.324656Z","shell.execute_reply":"2025-03-11T16:57:35.956400Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define base path\nbase_path = '/kaggle/input/comodity-price-prediction-penyisihan-arkavidia-9/Google Trend'\n\ngoogle_trend_filenames = os.listdir(base_path)\ngoogle_trend_dfs = dict()\n\nfor folder in google_trend_filenames:\n    commodity_df = None  \n    \n    folder_path = os.path.join(base_path, folder)\n    for province_file in os.listdir(folder_path):\n        province_path = os.path.join(folder_path, province_file)\n        province_df = pd.read_csv(province_path, parse_dates=['Date'])\n        \n        # Extract province name from filename (remove .csv)\n        province_name = province_file.rstrip('.csv')\n        \n        # Keep only relevant columns\n        province_df = province_df[['Date', folder]].rename(columns={folder: province_name})\n        \n        # Merge with main DataFrame\n        if commodity_df is None:\n            commodity_df = province_df  # Initialize with first province data\n        else:\n            commodity_df = commodity_df.merge(province_df, on='Date', how='outer')  # Merge on Date\n    \n    # Store in dictionary \n    google_trend_dfs[folder] = commodity_df\n    \n    # Print results\n    print('------------------------------------------------')\n    print(\"GOOGLE TREND ON \" + folder)\n    print(commodity_df.isna().agg(lambda x : np.round(sum(x)/len(commodity_df) * 100,2)).sort_values(ascending=False))\n    print('|||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||')\n    print(commodity_df)  # Print only the first few rows\n","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:35.960193Z","iopub.execute_input":"2025-03-11T16:57:35.960635Z","iopub.status.idle":"2025-03-11T16:57:42.486822Z","shell.execute_reply.started":"2025-03-11T16:57:35.960594Z","shell.execute_reply":"2025-03-11T16:57:42.485779Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Global Commodity","metadata":{}},{"cell_type":"code","source":"commodity_path = '/kaggle/input/comodity-price-prediction-penyisihan-arkavidia-9/Global Commodity Price'\ncommodity_filenames = os.listdir(commodity_path)\n\ncommodity = dict()\nfor file in commodity_filenames:\n    key_name = file.replace(' Historical Data.csv','')\n    print(key_name)\n    commodity[key_name] = pd.read_csv(os.path.join(commodity_path,file))\n    commodity[key_name]['Date'] = pd.to_datetime(commodity[key_name]['Date'])\n    commodity[key_name] = commodity[key_name].rename(columns={'Price': f\"commodity_price\"})\n    numerical_columns = ['Open','High','Low', 'commodity_price']\n    for nums in numerical_columns:\n        commodity[key_name][nums] =commodity[key_name][nums].astype(str).str.replace(\",\", \"\").astype(float) \n    print(commodity[key_name].isna().agg(lambda x : np.round(sum(x)/len(commodity[key_name]) * 100,2)).sort_values(ascending=False))\n    print(commodity[key_name].info())","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:42.488476Z","iopub.execute_input":"2025-03-11T16:57:42.488763Z","iopub.status.idle":"2025-03-11T16:57:42.650319Z","shell.execute_reply.started":"2025-03-11T16:57:42.488739Z","shell.execute_reply":"2025-03-11T16:57:42.649119Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Currency","metadata":{}},{"cell_type":"code","source":"uang_path = '/kaggle/input/comodity-price-prediction-penyisihan-arkavidia-9/Mata Uang'\nfilenames = os.listdir(uang_path)\n\nuang = dict()\nfor file in filenames:\n    uang[file[:6]] = pd.read_csv(os.path.join(uang_path,file))\n    uang[file[:6]]['Date'] = pd.to_datetime(uang[file[:6]]['Date'])\n    print(file[:6])\n    print(uang[file[:6]].isna().agg(lambda x : np.round(sum(x)/len(uang[file[:6]]) * 100,2)).sort_values(ascending=False))\n    print(uang[file[:6]].info())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:42.651317Z","iopub.execute_input":"2025-03-11T16:57:42.651640Z","iopub.status.idle":"2025-03-11T16:57:42.731322Z","shell.execute_reply.started":"2025-03-11T16:57:42.651615Z","shell.execute_reply":"2025-03-11T16:57:42.730384Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Description\n---","metadata":{"papermill":{"duration":0.013652,"end_time":"2025-03-01T18:11:42.781389","exception":false,"start_time":"2025-03-01T18:11:42.767737","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Whole Dataset Shape\n| **Train Folder**                        | **Google Trends**                     | **Global Commodity Price**       | **Mata Uang**   |\n|-----------------------------------------|---------------------------------------|----------------------------------|-----------------|\n| Daging Ayam Ras.csv                     |  daging ayam                          | crude oil futures                | MYRUSD          |\n| Daging Sapi Murni.csv                   |     daging sapi                    | natural gas futures              | SGDUSD          |\n| Telur Ayam Ras.csv                      |      telur ayam                           | newcastle coal futures           | THBUSD          |\n| Cabai Rawit Merah.csv                   | cabai rawit                                | palm oil futures                 | USDIDR          |\n| Gula Konsumsi.csv                       | bawang putih                          | us sugar 11 futures              |                 |\n| Beras Premium.csv                       | tepung terigu                         | us wheat futures                 |                 |\n| Tepung Terigu (Curah).csv               |  tepung                              |                                  |                 |\n| Beras Medium.csv                        |  beras                          |                                  |                 |\n| Minyak Goreng Curah.csv                 |  bawang                           |                                  |                 |\n| Bawang Merah.csv                        | cabai merah                           |                                  |                 |\n| Minyak Goreng Kemasan Sederhana.csv     | minyak goreng                         |                                  |                 |\n| Bawang Putih Bonggol.csv                | daging                                |                                  |                 |\n| Cabai Merah Keriting.csv                | cabai                                 |                                  |                 |\n|                                         | gula                                  |                                  |                 |\n|                                         | bawang merah                          |                                  |                 |","metadata":{}},{"cell_type":"markdown","source":"## Train","metadata":{"papermill":{"duration":0.013653,"end_time":"2025-03-01T18:11:42.809153","exception":false,"start_time":"2025-03-01T18:11:42.795500","status":"completed"},"tags":[]}},{"cell_type":"code","source":"for key, data in train.items():\n    print('--------------------------------------------')\n    print(key)\n    print(\"Start Date: \", min(data['Date']))\n    print(\"End Date: \", max(data['Date']))","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:42.732296Z","iopub.execute_input":"2025-03-11T16:57:42.732603Z","iopub.status.idle":"2025-03-11T16:57:42.760431Z","shell.execute_reply.started":"2025-03-11T16:57:42.732578Z","shell.execute_reply":"2025-03-11T16:57:42.759384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for key, data in train.items():\n    print(\"---------------------------------------------------------------\")\n    print(\"DESCRIBING \"+key)\n    print(data.describe())","metadata":{"papermill":{"duration":0.070583,"end_time":"2025-03-01T18:11:42.893667","exception":false,"start_time":"2025-03-01T18:11:42.823084","status":"completed"},"scrolled":true,"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:42.761656Z","iopub.execute_input":"2025-03-11T16:57:42.762049Z","iopub.status.idle":"2025-03-11T16:57:43.563503Z","shell.execute_reply.started":"2025-03-11T16:57:42.762007Z","shell.execute_reply":"2025-03-11T16:57:43.562419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for key, data in train.items():\n    print(\"---------------------------------------------------------------\")\n    print(\"NULL PERCENTAGE ON \"+key)\n    print(data.isna().agg(lambda x : np.round(sum(x)/len(data) * 100,2)).sort_values(ascending=False))","metadata":{"papermill":{"duration":0.05079,"end_time":"2025-03-01T18:11:42.959488","exception":false,"start_time":"2025-03-01T18:11:42.908698","status":"completed"},"scrolled":true,"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:43.564597Z","iopub.execute_input":"2025-03-11T16:57:43.564975Z","iopub.status.idle":"2025-03-11T16:57:43.663511Z","shell.execute_reply.started":"2025-03-11T16:57:43.564934Z","shell.execute_reply":"2025-03-11T16:57:43.662446Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Test","metadata":{"papermill":{"duration":0.015574,"end_time":"2025-03-01T18:11:43.163123","exception":false,"start_time":"2025-03-01T18:11:43.147549","status":"completed"},"tags":[]}},{"cell_type":"code","source":"for key, data in test.items():\n    print('--------------------------------------------')\n    print(key)\n    print(\"Start Date: \", min(data['Date']))\n    print(\"End Date: \", max(data['Date']))\n    break","metadata":{"papermill":{"duration":0.031615,"end_time":"2025-03-01T18:11:43.210414","exception":false,"start_time":"2025-03-01T18:11:43.178799","status":"completed"},"scrolled":true,"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:43.665791Z","iopub.execute_input":"2025-03-11T16:57:43.666071Z","iopub.status.idle":"2025-03-11T16:57:43.673390Z","shell.execute_reply.started":"2025-03-11T16:57:43.666047Z","shell.execute_reply":"2025-03-11T16:57:43.672242Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Google Trend","metadata":{}},{"cell_type":"code","source":"for key, data in google_trend_dfs.items():\n    print('--------------------------------------------')\n    print(key)\n    print(\"Start Date: \", min(data['Date']))\n    print(\"End Date: \", max(data['Date']))\n    break","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:43.674773Z","iopub.execute_input":"2025-03-11T16:57:43.675211Z","iopub.status.idle":"2025-03-11T16:57:43.701096Z","shell.execute_reply.started":"2025-03-11T16:57:43.675164Z","shell.execute_reply":"2025-03-11T16:57:43.699921Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Global Commodity Prices","metadata":{}},{"cell_type":"code","source":"for key, data in commodity.items():\n    print('--------------------------------------------')\n    print(key)\n    print(\"Start Date: \", min(data['Date']))\n    print(\"End Date: \", max(data['Date']))\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:43.702247Z","iopub.execute_input":"2025-03-11T16:57:43.702670Z","iopub.status.idle":"2025-03-11T16:57:43.724054Z","shell.execute_reply.started":"2025-03-11T16:57:43.702640Z","shell.execute_reply":"2025-03-11T16:57:43.723178Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Currency","metadata":{}},{"cell_type":"code","source":"for key, data in uang.items():\n    print('--------------------------------------------')\n    print(key)\n    print(\"Start Date: \", min(data['Date']))\n    print(\"End Date: \", max(data['Date']))\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:43.725035Z","iopub.execute_input":"2025-03-11T16:57:43.725361Z","iopub.status.idle":"2025-03-11T16:57:43.745319Z","shell.execute_reply.started":"2025-03-11T16:57:43.725332Z","shell.execute_reply":"2025-03-11T16:57:43.744094Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Preprocessing\n---","metadata":{"papermill":{"duration":0.017523,"end_time":"2025-03-01T18:11:43.443077","exception":false,"start_time":"2025-03-01T18:11:43.425554","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## ChatGPT Recommendations\n\n\n| **Commodity (Train Folder)** | **Google Trends (Demand Indicator)** | **Global Commodity Prices (Macroeconomic Factors)** | **Currency Exchange Rates (Import/Export Influence)** |\n|-----------------------------|--------------------------------------|-----------------------------------------------------|-----------------------------------------------------|\n| **Daging Ayam Ras** (Chicken Meat) | daging ayam, daging | None (but feed prices could be a factor) | USDIDR (feed imports), THBUSD (regional suppliers) |\n| **Daging Sapi Murni** (Beef) | daging sapi, daging | None | USDIDR (imported beef), MYRUSD, THBUSD (regional suppliers) |\n| **Telur Ayam Ras** (Chicken Eggs) | telur ayam | None | USDIDR (feed imports) |\n| **Cabai Rawit Merah** (Red Bird's Eye Chili) | cabai rawit, cabai merah, cabai | None | USDIDR (fertilizer imports) |\n| **Cabai Merah Keriting** (Curly Red Chili) | cabai merah, cabai rawit, cabai | None | USDIDR (fertilizer imports) |\n| **Gula Konsumsi** (Sugar) | gula | **US Sugar #11 Futures** | USDIDR (imported sugar) |\n| **Beras Premium** (Premium Rice) | beras | None (Rice is mostly domestically produced) | USDIDR (some imported rice), THBUSD (Thai rice market) |\n| **Beras Medium** (Medium Rice) | beras | None | USDIDR (some imported rice), THBUSD |\n| **Tepung Terigu (Curah)** (Wheat Flour) | tepung, tepung terigu | **US Wheat Futures** | USDIDR (imported wheat), MYRUSD (regional wheat suppliers) |\n| **Minyak Goreng Curah** (Bulk Cooking Oil) | minyak goreng | **Palm Oil Futures** | USDIDR, MYRUSD (Malaysia = major palm oil exporter) |\n| **Minyak Goreng Kemasan Sederhana** (Packaged Cooking Oil) | minyak goreng | **Palm Oil Futures** | USDIDR, MYRUSD |\n| **Bawang Merah** (Shallots) | bawang merah, bawang | None | USDIDR (imported fertilizers) |\n| **Bawang Putih Bonggol** (Garlic) | bawang putih, bawang | None | USDIDR (imported garlic, mostly from China) |","metadata":{}},{"cell_type":"code","source":"commodity_features = {\n    \"Daging Ayam Ras.csv\": {\n        \"google_trends\": [\"daging ayam\", \"daging\"],\n        \"global_commodity_prices\": [],\n        \"currency_exchange\": [\"USDIDR\", \"THBUSD\"]\n    },\n    \"Daging Sapi Murni.csv\": {\n        \"google_trends\": [\"daging sapi\", \"daging\"],\n        \"global_commodity_prices\": [],\n        \"currency_exchange\": [\"USDIDR\", \"MYRUSD\", \"THBUSD\"]\n    },\n    \"Telur Ayam Ras.csv\": {\n        \"google_trends\": [\"telur ayam\"],\n        \"global_commodity_prices\": [],\n        \"currency_exchange\": [\"USDIDR\"]\n    },\n    \"Cabai Rawit Merah.csv\": {\n        \"google_trends\": [\"cabai rawit\", \"cabai merah\", \"cabai\"],\n        \"global_commodity_prices\": [],\n        \"currency_exchange\": [\"USDIDR\"]\n    },\n    \"Cabai Merah Keriting.csv\": {\n        \"google_trends\": [\"cabai merah\", \"cabai rawit\", \"cabai\"],\n        \"global_commodity_prices\": [],\n        \"currency_exchange\": [\"USDIDR\"]\n    },\n    \"Gula Konsumsi.csv\": {\n        \"google_trends\": [\"gula\"],\n        \"global_commodity_prices\": [\"US Sugar 11 Futures\"],\n        \"currency_exchange\": [\"USDIDR\"]\n    },\n    \"Beras Premium.csv\": {\n        \"google_trends\": [\"beras\"],\n        \"global_commodity_prices\": [],\n        \"currency_exchange\": [\"USDIDR\", \"THBUSD\"]\n    },\n    \"Beras Medium.csv\": {\n        \"google_trends\": [\"beras\"],\n        \"global_commodity_prices\": [],\n        \"currency_exchange\": [\"USDIDR\", \"THBUSD\"]\n    },\n    \"Tepung Terigu (Curah).csv\": {\n        \"google_trends\": [\"tepung\", \"tepung terigu\"],\n        \"global_commodity_prices\": [\"US Wheat Futures\"],\n        \"currency_exchange\": [\"USDIDR\", \"MYRUSD\"]\n    },\n    \"Minyak Goreng Curah.csv\": {\n        \"google_trends\": [\"minyak goreng\"],\n        \"global_commodity_prices\": [\"Palm Oil Futures\"],\n        \"currency_exchange\": [\"USDIDR\", \"MYRUSD\"]\n    },\n    \"Minyak Goreng Kemasan Sederhana.csv\": {\n        \"google_trends\": [\"minyak goreng\"],\n        \"global_commodity_prices\": [\"Palm Oil Futures\"],\n        \"currency_exchange\": [\"USDIDR\", \"MYRUSD\"]\n    },\n    \"Bawang Merah.csv\": {\n        \"google_trends\": [\"bawang merah\", \"bawang\"],\n        \"global_commodity_prices\": [],\n        \"currency_exchange\": [\"USDIDR\"]\n    },\n    \"Bawang Putih Bonggol.csv\": {\n        \"google_trends\": [\"bawang putih\", \"bawang\"],\n        \"global_commodity_prices\": [],\n        \"currency_exchange\": [\"USDIDR\"]\n    }\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:43.746494Z","iopub.execute_input":"2025-03-11T16:57:43.746880Z","iopub.status.idle":"2025-03-11T16:57:43.765961Z","shell.execute_reply.started":"2025-03-11T16:57:43.746841Z","shell.execute_reply":"2025-03-11T16:57:43.764987Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Pivot Tables and Null Treatment","metadata":{}},{"cell_type":"markdown","source":"Steps:\n- Turn columns into row values (province)\n- Interpolate missing data","metadata":{}},{"cell_type":"code","source":"train_ready = dict()\nfor key, dataframes in train.items():\n    # Convert Date column to datetime format\n    dataframes[\"Date\"] = pd.to_datetime(dataframes[\"Date\"])\n    \n    # Melt the data to long format\n    dataframes_long = dataframes.melt(id_vars=[\"Date\"], var_name=\"Region\", value_name=\"Price\")\n\n    # Interpolate missing price values\n    dataframes_long[\"Price\"] = dataframes_long[\"Price\"].interpolate(method=\"linear\")\n    dataframes_long[\"Price\"] = dataframes_long[\"Price\"].bfill()\n    \n    # Check for missing values\n    missing_values = dataframes_long.isna().sum()\n    \n    # Display the transformed data and missing values\n    print('--------------------------------------------')\n    print(\"NULL VALUES on\", key)\n    print(missing_values)\n    print(dataframes_long.head())\n    train_ready[key] = dataframes_long","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:43.766812Z","iopub.execute_input":"2025-03-11T16:57:43.767057Z","iopub.status.idle":"2025-03-11T16:57:44.009223Z","shell.execute_reply.started":"2025-03-11T16:57:43.767037Z","shell.execute_reply":"2025-03-11T16:57:44.008163Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"google_trend_dfs_ready = dict()\nfor key, dataframes in google_trend_dfs.items():\n    # Convert Date column to datetime format\n    dataframes[\"Date\"] = pd.to_datetime(dataframes[\"Date\"])\n    \n    # Melt the data to long format\n    dataframes_long = dataframes.melt(id_vars=[\"Date\"], var_name=\"Region\", value_name=\"Searches\")\n\n    # Interpolate missing price values\n    dataframes_long[\"Searches\"] = dataframes_long[\"Searches\"].interpolate(method=\"linear\")\n    dataframes_long[\"Searches\"] = dataframes_long[\"Searches\"].bfill()\n    \n    # Check for missing values\n    missing_values = dataframes_long.isna().sum()\n    \n    # Display the transformed data and missing values\n    print('--------------------------------------------')\n    print(\"NULL VALUES on\", key)\n    print(missing_values)\n    print(dataframes_long.head())\n    \n    google_trend_dfs_ready[key] = dataframes_long","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-11T16:57:44.011524Z","iopub.execute_input":"2025-03-11T16:57:44.011841Z","iopub.status.idle":"2025-03-11T16:57:44.347399Z","shell.execute_reply.started":"2025-03-11T16:57:44.011812Z","shell.execute_reply":"2025-03-11T16:57:44.345970Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA\n---","metadata":{"papermill":{"duration":0.021259,"end_time":"2025-03-01T18:11:46.557573","exception":false,"start_time":"2025-03-01T18:11:46.536314","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Latest Commodity Prices per Province","metadata":{}},{"cell_type":"code","source":"i = 0\nsns.set_style('whitegrid')\nf, ax = plt.subplots(nrows=5, ncols=3, figsize=(25, 40))\n\nfor commodity, data in train_ready.items():\n    latest_price = data.groupby('Region').last()\n    latest_price = latest_price.reset_index()\n    latest_price = latest_price[['Region', 'Price']]\n    latest_price = latest_price.sort_values(by='Price', ascending=False)\n    ax[i//3, i%3].set_title('Price of ' + commodity)\n    ax[i//3, i%3].xaxis.set_tick_params(rotation=40)\n    sns.barplot(y='Region', x='Price', data=latest_price, ax=ax[i//3, i%3])\n    i+=1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:14:15.576884Z","iopub.status.idle":"2025-03-10T12:14:15.577274Z","shell.execute_reply":"2025-03-10T12:14:15.577092Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Latest Median Prices per Commodities","metadata":{}},{"cell_type":"code","source":"median_prices = dict()\nfor commodity, data in train_ready.items():\n    median_prices[commodity] = data['Price'].median()\nmedian_prices = pd.DataFrame(median_prices.items(), columns=['Commodity', 'Price'])\nmedian_prices = median_prices.sort_values(by='Price', ascending=False)\nplt.figure(figsize=(10, 5))\nplt.title('Median prices of 30-09-2024')\nsns.barplot(y='Commodity', x='Price', data=median_prices)\nplt.xticks(rotation=90)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:17:13.863399Z","iopub.execute_input":"2025-03-10T12:17:13.863704Z","iopub.status.idle":"2025-03-10T12:17:14.164591Z","shell.execute_reply.started":"2025-03-10T12:17:13.863681Z","shell.execute_reply":"2025-03-10T12:17:14.163778Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## TimeSeries Plot (Median)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Function to calculate EMA\ndef calculate_ema(series, span=30):\n    return series.ewm(span=span, adjust=False).mean()\n\nsns.set_style('whitegrid')\nf, ax = plt.subplots(nrows=5, ncols=3, figsize=(15, 25))\nax = ax.flatten()  # Flatten for easier indexing\n\ni = 0\nfor commodity, data in train_ready.items():\n    latest_price = data.groupby('Date', as_index=False)['Price'].median()\n    latest_price['EMA'] = calculate_ema(latest_price['Price'], span=30)  # Ensure consistency\n\n    # Handle Inf values\n    latest_price.replace([np.inf, -np.inf], np.nan, inplace=True)\n    latest_price.dropna(inplace=True)\n\n    ax[i].set_title('Price of ' + commodity)\n    \n    # Plot Actual Prices\n    sns.lineplot(x='Date', y='Price', data=latest_price, label='Actual', linewidth=2, ax=ax[i])\n\n    # Plot EMA\n    sns.lineplot(x='Date', y='EMA', data=latest_price, label='EMA (30-day)', \n                 linewidth=2, linestyle='dashed', color='red', ax=ax[i])\n\n    i += 1\n    if i >= len(ax):  # Stop if there are more commodities than subplots\n        break\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-10T12:17:15.086821Z","iopub.execute_input":"2025-03-10T12:17:15.087108Z","iopub.status.idle":"2025-03-10T12:17:21.825281Z","shell.execute_reply.started":"2025-03-10T12:17:15.087087Z","shell.execute_reply":"2025-03-10T12:17:21.824451Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Prediction for Testing\n---\n## Mata Uang","metadata":{}},{"cell_type":"code","source":"uang_modified = dict()\nfor key, df in uang.items():\n    # Convert Date column to datetime and sort\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n    df = df.sort_values(by=\"Date\")\n    \n    # Create moving averages\n    df[\"Close H+1\"] = df[\"Close\"].shift(periods= -1)\n    df[\"MA7\"] = df[\"Close\"].rolling(window=7).mean()\n    df[\"MA30\"] = df[\"Close\"].rolling(window=30).mean()\n    # Create rolling standard deviation (volatility)\n    df[\"Volatility7\"] = df[\"Close\"].rolling(window=7).std()\n\n    # Extract time-based features\n    df[\"DayOfWeek\"] = df[\"Date\"].dt.weekday\n    # df[\"Week\"] = df[\"Date\"].dt.isocalendar().week\n    df[\"Month\"] = df[\"Date\"].dt.month\n    # df[\"Quarter\"] = df[\"Date\"].dt.quarter\n    # df[\"Year\"] = df[\"Date\"].dt.year\n\n    df.drop(columns=[\"Adj Close\", \"Volume\", \"Date\", \"High\", \"Low\", \"Open\"], inplace=True)\n    \n    # Drop NaN values caused by shifting and rolling operations\n    df.dropna(inplace=True)\n    uang_modified[key] = df.copy()\n    print(\"---------------------------------------\")\n    print(key)\n    print(uang_modified[key].head(3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:08:36.857378Z","iopub.execute_input":"2025-03-10T15:08:36.857761Z","iopub.status.idle":"2025-03-10T15:08:36.926287Z","shell.execute_reply.started":"2025-03-10T15:08:36.857729Z","shell.execute_reply":"2025-03-10T15:08:36.924487Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to make the sequences\ndef create_sequences(data, seq_length=10):\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data[i:i + seq_length].drop(columns=[\"Close H+1\"]))\n        y.append(data[i:i + seq_length][\"Close H+1\"])\n    return np.array(X), np.array(y)\n\n# Function to update sequence\ndef update_sequence(last_sequence, pred, col_indices):\n    global last_known_date\n    last_known_date += pd.Timedelta(days=1)\n    new_row = last_sequence[1:].copy()\n    new_row = np.append(new_row, last_sequence[-1:].copy(), axis=0)\n    \n    new_row[0,-1, col_indices[\"Close\"]] = pred\n    new_row[0,-1, col_indices[\"MA7\"]] = np.mean(last_sequence[0, -7:, col_indices[\"Close\"]])\n    new_row[0,-1, col_indices[\"MA30\"]] = np.mean(last_sequence[0, -30:, col_indices[\"Close\"]])\n    new_row[0,-1, col_indices[\"Volatility7\"]] = np.std(last_sequence[0, -7:, col_indices[\"Close\"]])    \n    new_row[0,-1,col_indices[\"DayOfWeek\"]] = last_known_date.dayofweek\n    new_row[0,-1,col_indices[\"Month\"]] = last_known_date.month\n    # new_row[0,-1,col_indices[\"Quarter\"]] = last_known_date.quarter\n    # new_row[0,-1,col_indices[\"Year\"]] = last_known_date.year\n\n    return new_row","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:08:42.438824Z","iopub.execute_input":"2025-03-10T15:08:42.439230Z","iopub.status.idle":"2025-03-10T15:08:42.449110Z","shell.execute_reply.started":"2025-03-10T15:08:42.439177Z","shell.execute_reply":"2025-03-10T15:08:42.447858Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"uang_predicted = dict()\n\nfor key, df in uang_modified.items():\n    columns = list(df.columns)\n    panjang_sequence = 30\n    \n    train_size_uang = int(len(df)*0.8)\n    train_uang = df[:train_size_uang]\n    test_uang = df[train_size_uang:]\n    \n    scaler = MinMaxScaler()\n    train_scaled_uang = pd.DataFrame(scaler.fit_transform(train_uang), columns=columns)\n    test_scaled_uang = pd.DataFrame(scaler.transform(test_uang), columns=columns)\n\n    X_train_uang, y_train_uang = create_sequences(train_scaled_uang, panjang_sequence)\n    X_test_uang, y_test_uang = create_sequences(test_scaled_uang, panjang_sequence)\n    \n    # Define the LSTM model\n    jumlah_fitur = X_train_uang.shape[2]\n    input_layer = tf.keras.Input(shape=(panjang_sequence, jumlah_fitur))\n    lstm = tf.keras.layers.LSTM(50, return_sequences=True)(input_layer)\n    lstm = tf.keras.layers.Dropout(0.2, seed=RANDOM_STATE)(lstm)\n    lstm = tf.keras.layers.LSTM(25, return_sequences=False)(lstm)\n    lstm = tf.keras.layers.Dense(10, activation='relu')(lstm)\n    output_layer = tf.keras.layers.Dense(1)(lstm)\n\n    # Build the Model\n    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n    model.compile(\n        optimizer='adam', \n        loss='mse', \n        metrics=['mse'])\n\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        patience=5,\n        restore_best_weights=True\n    )\n    history = model.fit(X_train_uang, y_train_uang,\n                        batch_size=16,\n                        epochs=100,\n                        validation_data=(X_test_uang, y_test_uang),\n                        callbacks=[early_stop], verbose=0)\n\n    # Prediksi 92 hari ke depan\n    jumlah_hari_forecast = 92\n    last_sequence = np.array(test_scaled_uang[-panjang_sequence:].drop(columns=[\"Close\"])).reshape(1, panjang_sequence, -1)\n    last_known_date = pd.Timestamp(\"2024-09-30\")\n    col_indices = {\n        \"Close\": 0,\n        \"MA7\": 1,\n        \"MA30\": 2,\n        \"Volatility7\": 3,\n        \"DayOfWeek\": 4,\n        \"Month\": 5\n    }\n    predictions = []\n\n    for _ in range(jumlah_hari_forecast):\n        pred = model.predict(last_sequence)[0,0]\n        predictions.append(pred)\n        \n        # Update the sequence\n        last_sequence = update_sequence(last_sequence, pred, col_indices)\n\n    predictions_real = np.array(predictions, dtype=np.float64) * scaler.data_range_[col_indices[\"Close\"]] + scaler.data_min_[col_indices[\"Close\"]]\n    \n    # Create future dates\n    future_dates = pd.date_range(start=\"2024-10-01\", periods=jumlah_hari_forecast, freq=\"D\")\n    \n    # Save predictions\n    uang_predicted[key] = pd.DataFrame({\"Date\": future_dates, \"Predicted_Close\": predictions_real})","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-10T15:10:33.561577Z","iopub.execute_input":"2025-03-10T15:10:33.561961Z","iopub.status.idle":"2025-03-10T15:12:18.533447Z","shell.execute_reply.started":"2025-03-10T15:10:33.561927Z","shell.execute_reply":"2025-03-10T15:12:18.532086Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Komoditas","metadata":{}},{"cell_type":"code","source":"commodity_copy = dict()\nfor key, df in commodity.items():\n    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n    df = df.sort_values(by=\"Date\")\n    df.rename(columns={\"commodity_price\": \"Close\"}, inplace=True)\n    df[\"Close\"] = df[\"Close\"].astype(str).str.replace(\",\", \"\").astype(float)\n    df[\"CloseH+1\"] = df[\"Close\"].shift(periods=1)\n    \n    # Create moving averages\n    df[\"MA7\"] = df[\"Close\"].rolling(window=7).mean()\n    df[\"MA30\"] = df[\"Close\"].rolling(window=30).mean()\n    \n    # Create rolling standard deviation (volatility)\n    df[\"Volatility7\"] = df[\"Close\"].rolling(window=7).std()\n\n    # Extract time-based features\n    df[\"DayOfWeek\"] = df[\"Date\"].dt.weekday\n    df[\"Month\"] = df[\"Date\"].dt.month\n    # df[\"Quarter\"] = df[\"Date\"].dt.quarter\n    # df[\"Year\"] = df[\"Date\"].dt.year\n\n    # Drop unnecessary columns if they exist\n    columns_to_drop = [\"Vol.\", \"Change %\", \"Date\", \"Open\", \"High\", \"Low\"]\n    df.drop(columns=[col for col in columns_to_drop if col in df.columns], inplace=True)\n\n    # Drop NaN values caused by rolling operations\n    df.dropna(inplace=True)\n\n    # Store the processed dataframe in a new dictionary\n    commodity_copy[key] = df.copy()\n\n    # Display processed data for debugging\n    print('===================================================================')\n    print(f\"Processed data for {key}\")\n    print(df.info())\n    print(df.head(3))","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-03-10T15:44:42.055614Z","iopub.execute_input":"2025-03-10T15:44:42.056049Z","iopub.status.idle":"2025-03-10T15:44:42.177156Z","shell.execute_reply.started":"2025-03-10T15:44:42.056016Z","shell.execute_reply":"2025-03-10T15:44:42.175918Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to make sequences with proper handling of missing values\ndef create_sequences(data, seq_length=10):\n    X, y = [], []\n    for i in range(len(data) - seq_length):\n        X.append(data.iloc[i:i + seq_length].drop(columns=[\"CloseH+1\"]).values)\n        y.append(data.iloc[i + seq_length][\"CloseH+1\"])\n    \n    return np.array(X), np.array(y)\n\n# Function to update sequence dynamically with new predictions\ndef update_sequence(last_sequence, pred, col_indices):\n    new_row = np.copy(last_sequence)\n\n    # Shift left\n    new_row[0, :-1, :] = new_row[0, 1:, :]  \n    new_row[0, -1, col_indices[\"Close\"]] = pred  \n\n    # Only update if the column exists\n    for col in [\"DayOfWeek\", \"Month\"]:\n        if col in col_indices and col_indices[col] < new_row.shape[2]:  \n            new_row[0, -1, col_indices[col]] = getattr(last_known_date, col.lower())\n    \n    new_row[0,-1, col_indices[\"MA7\"]] = np.mean(last_sequence[0, -7:, col_indices[\"Close\"]])\n    new_row[0,-1, col_indices[\"MA30\"]] = np.mean(last_sequence[0, -30:, col_indices[\"Close\"]])\n    new_row[0,-1, col_indices[\"Volatility7\"]] = np.std(last_sequence[0, -7:, col_indices[\"Close\"]])    \n    return new_row","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:44:47.918341Z","iopub.execute_input":"2025-03-10T15:44:47.918716Z","iopub.status.idle":"2025-03-10T15:44:47.928190Z","shell.execute_reply.started":"2025-03-10T15:44:47.918687Z","shell.execute_reply":"2025-03-10T15:44:47.926951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:44:52.523602Z","iopub.execute_input":"2025-03-10T15:44:52.523983Z","iopub.status.idle":"2025-03-10T15:44:52.530139Z","shell.execute_reply.started":"2025-03-10T15:44:52.523953Z","shell.execute_reply":"2025-03-10T15:44:52.528971Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"commodity_price_predicted = {}\n\n# Iterate through each commodity dataset\nfor key, df in commodity_copy.items():\n    columns = list(df.columns)\n    sequence_lenght = 30\n\n    # Train-test split (80-20)\n    train_size_commodity = int(len(df) * 0.8)\n    train_commodity = df[:train_size_commodity]\n    test_commodity = df[train_size_commodity:]\n\n    for col in df.columns:\n        df[col] = df[col].astype(str).str.replace(',', '').astype(float)\n\n        # Train-test split (80-20)\n        train_size_commodity = int(len(df) * 0.8)\n        train_commodity = df[:train_size_commodity]\n        test_commodity = df[train_size_commodity:]\n\n    # Apply MinMaxScaler\n    scaler = MinMaxScaler()\n    train_scaled_commodity = pd.DataFrame(scaler.fit_transform(train_commodity), columns=df.columns)\n    test_scaled_commodity = pd.DataFrame(scaler.transform(test_commodity), columns=df.columns)\n\n\n    # Feature Scaling\n    columns = list(df.columns)\n    scaler = MinMaxScaler()\n    train_scaled_commodity = pd.DataFrame(scaler.fit_transform(train_commodity), columns=columns)\n    test_scaled_commodity = pd.DataFrame(scaler.transform(test_commodity), columns=columns)\n\n\n    # Create sequences\n    X_train_commodity, y_train_commodity = create_sequences(train_scaled_commodity, sequence_lenght)\n    X_test_commodity, y_test_commodity = create_sequences(test_scaled_commodity, sequence_lenght)\n\n    # Define the LSTM model\n    jumlah_fitur = X_train_commodity.shape[2]\n    input_layer = tf.keras.Input(shape=(sequence_lenght, jumlah_fitur))\n    lstm = tf.keras.layers.LSTM(50, return_sequences=True)(input_layer)\n    lstm = tf.keras.layers.Dropout(0.2)(lstm)\n    lstm = tf.keras.layers.LSTM(25, return_sequences=False)(lstm)\n    lstm = tf.keras.layers.Dense(10, activation='relu')(lstm)\n    output_layer = tf.keras.layers.Dense(1)(lstm)\n\n    # Compile the model\n    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n    model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n\n    # Early stopping to prevent overfitting\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n\n    print(\"Fitting model...\")\n    # Train the model\n    history = model.fit(X_train_commodity, y_train_commodity,\n                        batch_size=16,\n                        epochs=100,\n                        validation_data=(X_test_commodity, y_test_commodity),\n                        callbacks=[early_stop], verbose=0)\n\n    # Predict next 92 days\n    jumlah_hari_forecast = 92\n    last_sequence = np.array(test_scaled_commodity[-sequence_lenght:].drop(columns=[\"Close\"])).reshape(1, sequence_lenght, -1)\n    last_known_date = pd.Timestamp(\"2024-10-01\")\n\n    col_indices = {col: idx for idx, col in enumerate(columns)}\n    predictions = []\n    \n    print(\"Predicting model...\")\n    for _ in range(jumlah_hari_forecast):\n        pred = model.predict(last_sequence, verbose=0)[0, 0]\n        predictions.append(pred)\n        last_sequence = update_sequence(last_sequence, pred, col_indices)\n\n    # Convert predictions back to original scale\n    predictions_real = np.array(predictions) * scaler.data_range_[col_indices[\"Close\"]] + scaler.data_min_[col_indices[\"Close\"]]\n\n    # Create future dates\n    future_dates = pd.date_range(start=\"2024-09-30\", periods=jumlah_hari_forecast, freq=\"D\")\n\n    # Store predictions\n    commodity_price_predicted[key] = pd.DataFrame({\"Date\": future_dates, \"commodity_price\": predictions_real})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:59:57.577945Z","iopub.execute_input":"2025-03-10T15:59:57.578342Z","iopub.status.idle":"2025-03-10T16:00:05.894643Z","shell.execute_reply.started":"2025-03-10T15:59:57.578316Z","shell.execute_reply":"2025-03-10T16:00:05.892751Z"},"jupyter":{"outputs_hidden":true},"collapsed":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Modelling\n---","metadata":{"papermill":{"duration":0.137678,"end_time":"2025-03-01T18:14:30.803124","exception":false,"start_time":"2025-03-01T18:14:30.665446","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Modelling Preprocessing","metadata":{"papermill":{"duration":0.13054,"end_time":"2025-03-01T18:14:31.069014","exception":false,"start_time":"2025-03-01T18:14:30.938474","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def create_features(df_input, feats):\n    df = df_input.copy()\n    df['lag_7'] = df.groupby('Region')['Price'].shift(7).bfill()\n    df['lag_30'] = df.groupby('Region')['Price'].shift(30).bfill()\n    df['rolling_7'] = df.groupby('Region')['Price'].transform(lambda x: x.rolling(7, min_periods=1).mean())\n    df['rolling_30'] = df.groupby('Region')['Price'].transform(lambda x: x.rolling(30, min_periods=1).mean())\n    \n    # Time-based features\n    df['month'] = df['Date'].dt.month\n    df['weekday'] = df['Date'].dt.weekday\n\n    # Features\n    global_comm_feats = feats['global_commodity_prices'] \n    currency_feats = feats['currency_exchange']\n\n    for col in global_comm_feats:\n        print(\"merging \", col)\n        df = df.merge(commodity[col][[\"Date\", \"commodity_price\"]], on=\"Date\", how=\"left\")\n        df.rename(columns={'commodity_price': f'commodity_price_{col}'},inplace=True)\n        df[f'commodity_price_{col}'] = df[f'commodity_price_{col}'].ffill()\n        df[f'commodity_price_{col}'] = df[f'commodity_price_{col}'].bfill()\n        \n    for col in currency_feats:\n        print(\"merging \", col)\n        df = df.merge(uang[col][[\"Date\", \"Adj Close\"]], on=\"Date\", how=\"left\")\n        df.rename(columns={'Adj Close': f'currency_{col}'},inplace=True)\n        df[f'currency_{col}'] = df[f'currency_{col}'].ffill()\n        df[f'currency_{col}'] = df[f'currency_{col}'].bfill()\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:38:00.843327Z","iopub.execute_input":"2025-03-11T13:38:00.843667Z","iopub.status.idle":"2025-03-11T13:38:00.853123Z","shell.execute_reply.started":"2025-03-11T13:38:00.843640Z","shell.execute_reply":"2025-03-11T13:38:00.851850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def forecast_next_92_days(df, model, target, last_date, commodity_type, region):\n    \"\"\"\n    Forecast the next 92 days from latest row of data\n    \"\"\"\n    predictions = []\n    last_known_data = df.iloc[-60:].copy()  # Take last 60 days for lag features\n\n    for i in range(92):\n        next_date = last_date + timedelta(days=i + 1)\n        new_row = last_known_data.iloc[-1:].copy()  # Use last row as base\n\n        # Update rolling features dynamically\n        new_row['lag_7'] = last_known_data[target].iloc[-7]\n        new_row['lag_30'] = last_known_data[target].iloc[-30]\n        new_row['rolling_7'] = last_known_data[target].iloc[-7:].mean()\n        new_row['rolling_30'] = last_known_data[target].iloc[-30:].mean()\n\n        # Time features\n        new_row['month'] = next_date.month\n        new_row['weekday'] = next_date.weekday()\n\n        # Corrected currency exchange feature lookup\n        for col in commodity_features[commodity_type]['currency_exchange']:\n            if last_date <= pd.to_datetime('2024-09-30'):\n                matching_row = uang[col].loc[uang[col]['Date'] == last_date, 'Close']\n            else:\n                matching_row = uang_predicted[col].loc[uang_predicted[col]['Date'] == last_date, 'Predicted_Close']\n            new_row[f'currency_{col}'] = matching_row.values[0]  # Extract single value\n            \n        # Corrected global commodity prices lookup\n        for col in commodity_features[commodity_type]['global_commodity_prices']:\n            if (last_date <= pd.to_datetime('2024-09-30')):\n                matching_row = commodity[col].loc[commodity[col]['Date'] == last_date, 'commodity_price']\n            else:    \n                matching_row = commodity_price_predicted[col].loc[commodity_price_predicted[col]['Date'] == last_date, 'commodity_price']\n            new_row[f'commodity_price_{col}'] = matching_row.values[0]\n            \n        \n        # Drop Unnecessary Data\n        new_row.drop(columns=[target, 'Region', 'Date'], inplace=True, errors='ignore')\n\n        # Predict next day\n        pred_price = model.predict(pd.DataFrame(new_row))[0]\n        \n        # Format the ID\n        prediction_id = f\"{commodity_type.replace('.csv','')}/{region}/{next_date.strftime('%Y-%m-%d')}\"\n        predictions.append((prediction_id, pred_price))\n\n        # Append to last_known_data\n        new_entry = pd.DataFrame({target: [pred_price]}, index=[next_date])\n        last_known_data = pd.concat([last_known_data, new_entry])\n\n    return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:38:03.511729Z","iopub.execute_input":"2025-03-11T13:38:03.512192Z","iopub.status.idle":"2025-03-11T13:38:03.523953Z","shell.execute_reply.started":"2025-03-11T13:38:03.512137Z","shell.execute_reply":"2025-03-11T13:38:03.522357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train and predict for each DataFrame in train_ready\ntarget = 'Price'\n\ndef train_pipeline(base_model):\n    \"\"\"\n    Forecast the next 92 days from latest row of data\n    params:\n    base_model = Model instantiator\n    \"\"\"\n    predictions_dict = {}\n    for key, df in train_ready.items():\n        print(f\"Processing {key}...\")\n        predictions_list = []\n        df = create_features(df, commodity_features[key])\n        for region in df['Region'].unique():\n            print(f\"Processing Region: {region}\")\n            \n            # Define Region Data\n            region_df = df[df['Region'] == region]\n            X = region_df.drop([target]+['Region','Date'],axis=1)\n            y = region_df[target]\n            \n            # Train model on full data\n            model = base_model            \n            model.fit(X, y)\n    \n            # Predict next 92 days\n            last_date = df['Date'].max()\n            pred_values = forecast_next_92_days(region_df, model, target, last_date, key, region)\n    \n            # Store predictions\n            predictions_list.append(pd.DataFrame(pred_values, columns=['id', 'Predicted Price']))\n    \n        predictions_dict[key] = pd.concat(predictions_list, ignore_index=True)\n    return predictions_dict","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T13:38:06.278679Z","iopub.execute_input":"2025-03-11T13:38:06.279086Z","iopub.status.idle":"2025-03-11T13:38:06.286321Z","shell.execute_reply.started":"2025-03-11T13:38:06.279052Z","shell.execute_reply":"2025-03-11T13:38:06.284927Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## XGBoost Hyperparameter\n---","metadata":{}},{"cell_type":"code","source":"# Prediction with Hyperparameter Tuning\npredictions_dict = dict()\ntarget = 'Price'\noverall_mape_scores = []\nfor key, df in train_ready.items():\n    print(f\"Processing {key}...\")\n    predictions_list = []\n    df = create_features(df, commodity_features[key])\n    missing_values = df.isna().sum()\n    print(missing_values)\n    df.dropna(inplace=True)\n    for region in df['Region'].unique():\n        print(f\"Processing Region: {region}\")\n        \n        # Define Region Group\n        region_df = df[df['Region'] == region]\n        X = region_df.drop([target]+['Region','Date'],axis=1)\n        y = region_df[target]\n\n        tscv = TimeSeriesSplit(n_splits=3)\n        \n        # Set model parameters\n        params = {\n            'learning_rate': [0.01, 0.05, 0.1],\n            'max_depth': [4, 6, 8],\n            'n_estimators': [300, 600, 1000],\n        }\n\n        # Train model on full data\n        model = XGBRegressor(random_state=RANDOM_STATE, tree_method='hist', objective='reg:squarederror')\n        grid = GridSearchCV(model, param_grid=params,\n                            scoring='neg_mean_absolute_percentage_error',\n                            cv=tscv,\n                            n_jobs=-1)\n        grid.fit(X, y)\n\n        best_params = grid.best_params_\n        best_model = XGBRegressor(**best_params, random_state=RANDOM_STATE, tree_method='hist', objective='reg:squarederror')\n        best_model.fit(X, y)\n\n        # Predict next 92 days\n        last_date = region_df['Date'].max()\n        pred_values = forecast_next_92_days(region_df, best_model, target, last_date, key, region)\n\n        # Store predictions\n        predictions_list.append(pd.DataFrame(pred_values, columns=['id', 'Predicted Price']))\n\n    predictions_dict[key] = pd.concat(predictions_list, ignore_index=True)\n\npredictions = predictions_dict","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T12:14:15.596309Z","iopub.status.idle":"2025-03-10T12:14:15.596598Z","shell.execute_reply":"2025-03-10T12:14:15.596475Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Random Forest Hyperparameter\n---","metadata":{}},{"cell_type":"code","source":"# TimeSeries Split CV test\npredictions_df = {}\nfeatures = ['lag_7', 'lag_30', 'rolling_7', 'rolling_30', 'month', 'weekday']\ntarget = 'Price'\noverall_mape_scores = []\n\n# Define the hyperparameter grid\nparam_dist = {\n    'n_estimators': [100, 300, 500, 1000],\n    'max_depth': [10, 15, 20, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': [0.5, 0.7, 'sqrt', 'log2'],\n    'bootstrap': [True, False]\n}\n\npredictions_dict = dict()\nfor key, df in train_ready.items():\n    print(f\"Processing {key}...\")\n    predictions_list = []\n    df = create_features(df, commodity_features[key])\n    \n    # Handle missing values\n    df.dropna(inplace=True)\n\n    for region in df['Region'].unique():\n        print(f\"Processing Region: {region}\")\n        \n        # Define Region Group\n        region_df = df[df['Region'] == region]\n        X = region_df.drop([target] + ['Region', 'Date'], axis=1)\n        y = region_df[target]\n\n        # TimeSeriesSplit Cross-Validation\n        tscv = TimeSeriesSplit(n_splits=5)\n        mape_scores = [] \n\n        # Run RandomizedSearchCV with TimeSeriesSplit\n        rf = RandomForestRegressor(random_state=42, n_jobs=-1)\n        random_search = RandomizedSearchCV(\n            estimator=rf, \n            param_distributions=param_dist, \n            n_iter=10,  # Try 10 different random combinations\n            cv=tscv, \n            verbose=1, \n            n_jobs=-1,\n            scoring='neg_mean_absolute_percentage_error'\n        )\n\n        # Fit hyperparameter tuning on training data\n        random_search.fit(X, y)\n\n        # Get best parameters\n        best_params = random_search.best_params_\n        print(f\"Best Parameters for {region}: {best_params}\")\n\n        # Train model with the best parameters\n        best_model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n        best_model.fit(X, y)\n    \n        # Predict next 92 days\n        last_date = df['Date'].max()\n        pred_values = forecast_next_92_days(region_df, best_model, target, last_date, key, region)\n\n        # Store predictions\n        predictions_list.append(pd.DataFrame(pred_values, columns=['id', 'Predicted Price']))\n    \n    predictions_dict[key] = pd.concat(predictions_list, ignore_index=True)\n\npredictions = predictions_dict","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ARIMA\n---","metadata":{}},{"cell_type":"code","source":"from statsmodels.tsa.arima.model import ARIMA\n\ndef train_pipeline(target, arima_order):\n    predictions_dict = {}\n    for key, df in train_ready.items():\n        print(f\"Processing {key}...\")\n        predictions_list = []\n        \n        df = create_features(df, commodity_features[key])\n        \n        # Convert Date column to datetime and set it as the index\n        df['Date'] = pd.to_datetime(df['Date'])\n        df.set_index('Date', inplace=True)\n        \n        for region in df['Region'].unique():\n            print(f\"Processing Region: {region}\")\n            \n            # Filter data for the region\n            region_df = df[df['Region'] == region]\n            y = region_df[target]\n            \n            # Train ARIMA model\n            model = ARIMA(y, order=arima_order)  # Corrected ARIMA initialization\n            model_fit = model.fit()\n            \n            # Predict next 92 days\n            last_date = y.index.max()\n            pred_values = model_fit.forecast(steps=92)  # Forecasting future prices\n            \n            # Create forecast dates\n            pred_dates = pd.date_range(start=last_date + timedelta(days=1), periods=92, freq='D')\n            \n            # Format predictions\n            pred_df = pd.DataFrame({\n                'id': [f\"{key}/{region}/{date.strftime('%Y-%m-%d')}\" for date in pred_dates],\n                'Predicted Price': pred_values\n            })\n            predictions_list.append(pred_df)\n    \n        predictions_dict[key] = pd.concat(predictions_list, ignore_index=True)\n    \n    return predictions_dict  # Return predictions & trained models\n\n# Example usage with ARIMA order (p=5, d=1, q=0)\narima_order = (5, 1, 0)  # You can tune these values\npredictions = train_pipeline(target='Price', arima_order=arima_order)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T13:05:50.137761Z","iopub.execute_input":"2025-03-10T13:05:50.138165Z","iopub.status.idle":"2025-03-10T13:06:57.224399Z","shell.execute_reply.started":"2025-03-10T13:05:50.138137Z","shell.execute_reply":"2025-03-10T13:06:57.223470Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Submission\n---","metadata":{"papermill":{"duration":0.379799,"end_time":"2025-03-01T18:30:27.791119","exception":false,"start_time":"2025-03-01T18:30:27.411320","status":"completed"},"tags":[]}},{"cell_type":"code","source":"prediction_final = pd.DataFrame(columns=['id','Predicted Price'])\nfor key, df in predictions.items():\n    print(df.head(3))\n    prediction_final = pd.concat([prediction_final, df])\nprediction_final = prediction_final.rename(columns={'Predicted Price':'price'})\nprint(prediction_final.head())\nprediction_final.to_csv('submission.csv',index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T22:16:23.638089Z","iopub.execute_input":"2025-03-11T22:16:23.638421Z","iopub.status.idle":"2025-03-11T22:16:23.827961Z","shell.execute_reply.started":"2025-03-11T22:16:23.638394Z","shell.execute_reply":"2025-03-11T22:16:23.826611Z"}},"outputs":[],"execution_count":null}]}